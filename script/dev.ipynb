{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3715fd9-5cbb-4ea3-843b-2992877601a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5ff5774-4047-4616-a6ff-9486c5f8c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"PySpark Exercise Week 6\") \\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48191af6-f58d-4281-9406-fc1643c85ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pyspark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Exercise Week 6</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1ac02579d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "752c8132-7e97-488d-83d2-2237a18906d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env and define the credentials\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "SOURCE_DB_HOST = os.getenv(\"SOURCE_DB_HOST\")\n",
    "SOURCE_DB_NAME = os.getenv(\"SOURCE_DB_NAME\")\n",
    "SOURCE_DB_USER = os.getenv(\"SOURCE_DB_USER\")\n",
    "SOURCE_DB_PASS = os.getenv(\"SOURCE_DB_PASS\")\n",
    "SOURCE_DB_PORT = os.getenv(\"SOURCE_DB_PORT\")\n",
    "\n",
    "DWH_DB_HOST = os.getenv(\"DWH_DB_HOST\")\n",
    "DWH_DB_NAME = os.getenv(\"DWH_DB_NAME\")\n",
    "DWH_DB_USER = os.getenv(\"DWH_DB_USER\")\n",
    "DWH_DB_PASS = os.getenv(\"DWH_DB_PASS\")\n",
    "DWH_DB_PORT = os.getenv(\"DWH_DB_PORT\")\n",
    "\n",
    "LOG_DB_HOST = os.getenv(\"LOG_DB_HOST\")\n",
    "LOG_DB_NAME = os.getenv(\"LOG_DB_NAME\")\n",
    "LOG_DB_USER = os.getenv(\"LOG_DB_USER\")\n",
    "LOG_DB_PASS = os.getenv(\"LOG_DB_PASS\")\n",
    "LOG_DB_PORT = os.getenv(\"LOG_DB_PORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0f57f8c-0719-47dd-9699-fa44711e711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_engine():\n",
    "    SOURCE_DB_URL = f\"jdbc:postgresql://{SOURCE_DB_HOST}:{SOURCE_DB_PORT}/{SOURCE_DB_NAME}\"\n",
    "    return SOURCE_DB_URL, SOURCE_DB_USER, SOURCE_DB_PASS \n",
    "\n",
    "def dwh_engine():\n",
    "    DWH_DB_URL = f\"jdbc:postgresql://{DWH_DB_HOST}:{DWH_DB_PORT}/{DWH_DB_NAME}\"\n",
    "    return DWH_DB_URL, DWH_DB_USER, DWH_DB_PASS \n",
    "\n",
    "def log_engine():\n",
    "    LOG_DB_URL = f\"jdbc:postgresql://{LOG_DB_HOST}:{LOG_DB_PORT}/{LOG_DB_NAME}\"\n",
    "    return LOG_DB_URL, LOG_DB_USER, LOG_DB_PASS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b445021-49c7-4f46-a466-ec2b43519359",
   "metadata": {},
   "source": [
    "## **Set Up Logging Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02ec70f0-04cb-452d-808b-2f0fdfc6126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def logging_process():\n",
    "    \n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        filename=\"/home/jovyan/work/log/info.log\",\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e83f0fe-5e12-4f84-a69c-9b6b25e2f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log_msg(spark: SparkSession, log_msg):\n",
    "\n",
    "    LOG_DB_URL, LOG_DB_USER, LOG_DB_PASS = log_engine()\n",
    "    table_name = \"etl_log\"\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": LOG_DB_USER,\n",
    "        \"password\": LOG_DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "\n",
    "    log_msg.write.jdbc(url = LOG_DB_URL,\n",
    "                  table = table_name,\n",
    "                  mode = \"append\",\n",
    "                  properties = connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab013d-d654-495a-a3a2-06d498ee2700",
   "metadata": {},
   "source": [
    "## **Extract Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbb3239c-d7cb-44af-9d90-b0ced1f2242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_database(spark: SparkSession, table_name: str):\n",
    "    \n",
    "    # Get source db config\n",
    "    SOURCE_DB_URL, SOURCE_DB_USER, SOURCE_DB_PASS = source_engine()\n",
    "\n",
    "    # Set config\n",
    "    connection_properties = {\n",
    "        \"user\" : SOURCE_DB_USER,\n",
    "        \"password\" : SOURCE_DB_PASS,\n",
    "        \"driver\" : \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    # Set current timestamp for logging\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Read data\n",
    "        df = spark \\\n",
    "            .read \\\n",
    "            .jdbc(url=SOURCE_DB_URL,\n",
    "                  table=table_name,\n",
    "                  properties=connection_properties)\n",
    "\n",
    "        print(f\"Extraction process successful for table: {table_name}\")\n",
    "\n",
    "        # Set success log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"extraction\", \"success\", \"source_db\", table_name, current_timestamp)]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\"])\n",
    "    \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction process failed: {e}\")\n",
    "\n",
    "        # Set failed log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"extraction\", \"failed\", \"source_db\", table_name, current_timestamp, str(e))]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "\n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0b3aa5a-a856-44d6-91f6-906b84d54c58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction process successful for table: education_status\n",
      "Extraction process successful for table: marital_status\n",
      "Extraction process successful for table: marketing_campaign_deposit\n"
     ]
    }
   ],
   "source": [
    "education_status_df = extract_database(spark=spark, table_name=\"education_status\")\n",
    "marital_status_df = extract_database(spark=spark, table_name=\"marital_status\")\n",
    "marketing_campaign_deposit_df = extract_database(spark=spark, table_name=\"marketing_campaign_deposit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5781398a-d1d3-4fbd-8c67-2cb0421914dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------------+--------------------+\n",
      "|education_id|    value|          created_at|          updated_at|\n",
      "+------------+---------+--------------------+--------------------+\n",
      "|           1| tertiary|2025-02-28 15:31:...|2025-02-28 15:31:...|\n",
      "|           2|secondary|2025-02-28 15:31:...|2025-02-28 15:31:...|\n",
      "|           3|  unknown|2025-02-28 15:31:...|2025-02-28 15:31:...|\n",
      "|           4|  primary|2025-02-28 15:31:...|2025-02-28 15:31:...|\n",
      "+------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "education_status_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1f4a243-e083-48f6-b277-0790c6f8951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+--------------------+\n",
      "|marital_id|   value|          created_at|          updated_at|\n",
      "+----------+--------+--------------------+--------------------+\n",
      "|         1| married|2025-02-28 15:31:...|2025-02-28 15:31:...|\n",
      "|         2|  single|2025-02-28 15:31:...|2025-02-28 15:31:...|\n",
      "|         3|divorced|2025-02-28 15:31:...|2025-02-28 15:31:...|\n",
      "+----------+--------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "marital_status_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4fbccf-f7ef-4200-8bbb-3c92c5d4b2e9",
   "metadata": {},
   "source": [
    "## **Extract CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff29cec7-92cc-4d9d-bdb7-fdd1ea638b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "\n",
    "def extract_csv(spark: SparkSession, file_name: str):\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "        # Read data\n",
    "        df = spark.read.csv(path + file_name, header=True)\n",
    "        \n",
    "        print(f\"Extraction process successful for file: {file_name}\")\n",
    "\n",
    "        # Set success log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"extraction\", \"success\", \"csv\", file_name, current_timestamp)]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\"])\n",
    "    \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction process failed: {e}\")\n",
    "\n",
    "        # Set failed log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"extraction\", \"failed\", \"csv\", file_name, current_timestamp, str(e))]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "\n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46a58e56-f2d8-4cd1-a652-97ca775cefe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction process successful for file: new_bank_transaction.csv\n"
     ]
    }
   ],
   "source": [
    "bank_df = extract_csv(spark=spark, file_name=\"new_bank_transaction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ce41a6-6927-4f2a-af47-4279cfed809c",
   "metadata": {},
   "source": [
    "## **Data Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90c854b7-4ab9-486c-b91b-9f9a71088530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Percentage of Missing Values for each column with pyspark\n",
    "import pandas as pd\n",
    "import json\n",
    "from pyspark.sql.functions import col, count, when, round\n",
    "\n",
    "def check_missing_values(df):\n",
    "\n",
    "    total_data = df.count()\n",
    "\n",
    "    # Calculate the percentage of missing values for each column\n",
    "    get_missing_values = df.select([\n",
    "        round((count(when(col(c).isNull(), c)) / total_data) * 100, 2).alias(c)\n",
    "        for c in df.columns\n",
    "    ]).collect()[0].asDict()\n",
    "    \n",
    "    return get_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8fa9768-2c36-4085-be29-799ab90dd447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"person_in_charge\": \"rico_febrian\",\n",
      "    \"checking_date\": \"23/03/25\",\n",
      "    \"Column Information\": {\n",
      "        \"Education Status\": {\n",
      "            \"count\": 4,\n",
      "            \"columns\": [\n",
      "                \"education_id\",\n",
      "                \"value\",\n",
      "                \"created_at\",\n",
      "                \"updated_at\"\n",
      "            ]\n",
      "        },\n",
      "        \"Marital Status\": {\n",
      "            \"count\": 4,\n",
      "            \"columns\": [\n",
      "                \"marital_id\",\n",
      "                \"value\",\n",
      "                \"created_at\",\n",
      "                \"updated_at\"\n",
      "            ]\n",
      "        },\n",
      "        \"Marketing Campaign Deposit\": {\n",
      "            \"count\": 20,\n",
      "            \"columns\": [\n",
      "                \"loan_data_id\",\n",
      "                \"age\",\n",
      "                \"job\",\n",
      "                \"marital_id\",\n",
      "                \"education_id\",\n",
      "                \"default\",\n",
      "                \"balance\",\n",
      "                \"housing\",\n",
      "                \"loan\",\n",
      "                \"contact\",\n",
      "                \"day\",\n",
      "                \"month\",\n",
      "                \"duration\",\n",
      "                \"campaign\",\n",
      "                \"pdays\",\n",
      "                \"previous\",\n",
      "                \"poutcome\",\n",
      "                \"subscribed_deposit\",\n",
      "                \"created_at\",\n",
      "                \"updated_at\"\n",
      "            ]\n",
      "        },\n",
      "        \"Bank Transaction\": {\n",
      "            \"count\": 9,\n",
      "            \"columns\": [\n",
      "                \"TransactionID\",\n",
      "                \"CustomerID\",\n",
      "                \"CustomerDOB\",\n",
      "                \"CustGender\",\n",
      "                \"CustLocation\",\n",
      "                \"CustAccountBalance\",\n",
      "                \"TransactionDate\",\n",
      "                \"TransactionTime\",\n",
      "                \"TransactionAmount (INR)\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"Check Data Size\": {\n",
      "        \"Education Status\": 4,\n",
      "        \"Marital Status\": 3,\n",
      "        \"Marketing Campaign Deposit\": 45211,\n",
      "        \"Bank Transaction\": 1048567\n",
      "    },\n",
      "    \"Data Type For Each Column\": {\n",
      "        \"education status\": [\n",
      "            [\n",
      "                \"education_id\",\n",
      "                \"int\"\n",
      "            ],\n",
      "            [\n",
      "                \"value\",\n",
      "                \"string\"\n",
      "            ],\n",
      "            [\n",
      "                \"created_at\",\n",
      "                \"timestamp\"\n",
      "            ],\n",
      "            [\n",
      "                \"updated_at\",\n",
      "                \"timestamp\"\n",
      "            ]\n",
      "        ],\n",
      "        \"marital status\": [\n",
      "            [\n",
      "                \"marital_id\",\n",
      "                \"int\"\n",
      "            ],\n",
      "            [\n",
      "                \"value\",\n",
      "                \"string\"\n",
      "            ],\n",
      "            [\n",
      "                \"created_at\",\n",
      "                \"timestamp\"\n",
      "            ],\n",
      "            [\n",
      "                \"updated_at\",\n",
      "                \"timestamp\"\n",
      "            ]\n",
      "        ],\n",
      "        \"marketing campaign deposit\": [\n",
      "            [\n",
      "                \"loan_data_id\",\n",
      "                \"int\"\n",
      "            ],\n",
      "            [\n",
      "                \"age\",\n",
      "                \"int\"\n",
      "            ],\n",
      "            [\n",
      "                \"job\",\n",
      "                \"string\"\n",
      "            ],\n",
      "            [\n",
      "                \"marital_id\",\n",
      "                \"int\"\n",
      "            ],\n",
      "            [\n",
      "                \"education_id\",\n",
      "                \"int\"\n",
      "            ],\n",
      "            [\n",
      "                \"default\",\n",
      "                \"boolean\"\n",
      "            ],\n",
      "            [\n",
      "                \"balance\",\n",
      "                \"string\"\n",
      "            ],\n",
      "            [\n",
      "                \"housing\",\n",
      "                \"boolean\"\n",
      "            ],\n",
      "            [\n",
      "                \"loan\",\n",
      "                \"boolean\"\n",
      "            ],\n",
      "            [\n",
      "                \"contact\",\n",
      "                \"string\"\n",
      "            ],\n",
      "            [\n",
      "                \"day\",\n",
      "                \"int\"\n",
      "            ],\n",
      "            [\n",
      "                \"month\",\n",
      "                \"string\"\n",
      "            ],\n",
      "            [\n",
      "                \"duration\",\n",
      "                \"int\"\n",
      "            ],\n",
      "            [\n",
      "                \"campaign\",\n",
      "                \"int\"\n",
      "            ],\n",
      "            [\n",
      "                \"pdays\",\n",
      "                \"int\"\n",
      "            ],\n",
      "            [\n",
      "                \"previous\",\n",
      "                \"int\"\n",
      "            ],\n",
      "            [\n",
      "                \"poutcome\",\n",
      "                \"string\"\n",
      "            ],\n",
      "            [\n",
      "                \"subscribed_deposit\",\n",
      "                \"boolean\"\n",
      "            ],\n",
      "            [\n",
      "                \"created_at\",\n",
      "                \"timestamp\"\n",
      "            ],\n",
      "            [\n",
      "                \"updated_at\",\n",
      "                \"timestamp\"\n",
      "            ]\n",
      "        ],\n",
      "        \"bank transaction\": [\n",
      "            [\n",
      "                \"TransactionID\",\n",
      "                \"string\"\n",
      "            ],\n",
      "            [\n",
      "                \"CustomerID\",\n",
      "                \"string\"\n",
      "            ],\n",
      "            [\n",
      "                \"CustomerDOB\",\n",
      "                \"string\"\n",
      "            ],\n",
      "            [\n",
      "                \"CustGender\",\n",
      "                \"string\"\n",
      "            ],\n",
      "            [\n",
      "                \"CustLocation\",\n",
      "                \"string\"\n",
      "            ],\n",
      "            [\n",
      "                \"CustAccountBalance\",\n",
      "                \"string\"\n",
      "            ],\n",
      "            [\n",
      "                \"TransactionDate\",\n",
      "                \"string\"\n",
      "            ],\n",
      "            [\n",
      "                \"TransactionTime\",\n",
      "                \"string\"\n",
      "            ],\n",
      "            [\n",
      "                \"TransactionAmount (INR)\",\n",
      "                \"string\"\n",
      "            ]\n",
      "        ]\n",
      "    },\n",
      "    \"Check Missing Value\": {\n",
      "        \"education status\": {\n",
      "            \"education_id\": 0.0,\n",
      "            \"value\": 0.0,\n",
      "            \"created_at\": 0.0,\n",
      "            \"updated_at\": 0.0\n",
      "        },\n",
      "        \"marital status\": {\n",
      "            \"marital_id\": 0.0,\n",
      "            \"value\": 0.0,\n",
      "            \"created_at\": 0.0,\n",
      "            \"updated_at\": 0.0\n",
      "        },\n",
      "        \"marketing campaign deposit\": {\n",
      "            \"loan_data_id\": 0.0,\n",
      "            \"age\": 0.0,\n",
      "            \"job\": 0.0,\n",
      "            \"marital_id\": 0.0,\n",
      "            \"education_id\": 0.0,\n",
      "            \"default\": 0.0,\n",
      "            \"balance\": 0.0,\n",
      "            \"housing\": 0.0,\n",
      "            \"loan\": 0.0,\n",
      "            \"contact\": 0.0,\n",
      "            \"day\": 0.0,\n",
      "            \"month\": 0.0,\n",
      "            \"duration\": 0.0,\n",
      "            \"campaign\": 0.0,\n",
      "            \"pdays\": 0.0,\n",
      "            \"previous\": 0.0,\n",
      "            \"poutcome\": 0.0,\n",
      "            \"subscribed_deposit\": 0.0,\n",
      "            \"created_at\": 0.0,\n",
      "            \"updated_at\": 0.0\n",
      "        },\n",
      "        \"bank transaction\": {\n",
      "            \"TransactionID\": 0.0,\n",
      "            \"CustomerID\": 0.0,\n",
      "            \"CustomerDOB\": 0.0,\n",
      "            \"CustGender\": 0.1,\n",
      "            \"CustLocation\": 0.01,\n",
      "            \"CustAccountBalance\": 0.23,\n",
      "            \"TransactionDate\": 0.0,\n",
      "            \"TransactionTime\": 0.0,\n",
      "            \"TransactionAmount (INR)\": 0.0\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "data_profiling_report = {\n",
    "    \"person_in_charge\" : \"rico_febrian\",\n",
    "    \"checking_date\" : datetime.now().strftime('%d/%m/%y'),\n",
    "    \"Column Information\": {\n",
    "        \"Education Status\": {\"count\": len(education_status_df.columns), \"columns\": education_status_df.columns},\n",
    "        \"Marital Status\": {\"count\": len(marital_status_df.columns), \"columns\": marital_status_df.columns},\n",
    "        \"Marketing Campaign Deposit\": {\"count\": len(marketing_campaign_deposit_df.columns), \"columns\": marketing_campaign_deposit_df.columns},\n",
    "        \"Bank Transaction\": {\"count\": len(bank_df.columns), \"columns\": bank_df.columns},\n",
    "    },\n",
    "    \"Check Data Size\": {\n",
    "        \"Education Status\": education_status_df.count(),\n",
    "        \"Marital Status\": marital_status_df.count(),\n",
    "        \"Marketing Campaign Deposit\": marketing_campaign_deposit_df.count(),\n",
    "        \"Bank Transaction\": bank_df.count(),\n",
    "    },\n",
    "    \"Data Type For Each Column\" : {\n",
    "        \"education status\": education_status_df.dtypes,\n",
    "        \"marital status\": marital_status_df.dtypes,\n",
    "        \"marketing campaign deposit\": marketing_campaign_deposit_df.dtypes,\n",
    "        \"bank transaction\": bank_df.dtypes\n",
    "    },\n",
    "    \"Check Missing Value\" : {\n",
    "        \"education status\": check_missing_values(education_status_df),\n",
    "        \"marital status\": check_missing_values(marital_status_df),\n",
    "        \"marketing campaign deposit\": check_missing_values(marketing_campaign_deposit_df),\n",
    "        \"bank transaction\": check_missing_values(bank_df)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print dalam format JSON yang rapi\n",
    "print(json.dumps(data_profiling_report, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6261335-f824-4662-a82f-3b73ca1a5356",
   "metadata": {},
   "source": [
    "## **Transform Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0761a6a6-7d38-4260-8805-ff6ea93e9603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------\n",
      " loan_data_id       | 1                    \n",
      " age                | 58                   \n",
      " job                | management           \n",
      " marital_id         | 1                    \n",
      " education_id       | 1                    \n",
      " default            | false                \n",
      " balance            | $2143                \n",
      " housing            | true                 \n",
      " loan               | false                \n",
      " contact            | unknown              \n",
      " day                | 5                    \n",
      " month              | may                  \n",
      " duration           | 261                  \n",
      " campaign           | 1                    \n",
      " pdays              | -1                   \n",
      " previous           | 0                    \n",
      " poutcome           | unknown              \n",
      " subscribed_deposit | false                \n",
      " created_at         | 2025-02-28 15:59:... \n",
      " updated_at         | 2025-02-28 15:59:... \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "marketing_campaign_deposit_df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc47a7e3-12e0-4b12-b86d-95d1877b0bfe",
   "metadata": {},
   "source": [
    "### Transform *marketing_campaign_deposit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcf7e78b-d02f-4e32-8ddb-8cd7bbf25308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def transform_marketing_campaign(spark, df):\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Rename selected columns\n",
    "        rename_columns = {\n",
    "            \"pdays\" : \"days_since_last_campaign\",\n",
    "            \"previous\" : \"previous_campaign_contacts\",\n",
    "            \"poutcome\" : \"previous_campaign_outcome\"\n",
    "        }\n",
    "        \n",
    "        df = df.withColumnsRenamed(rename_columns)\n",
    "\n",
    "        # Remove dollar ($) sign in balance column\n",
    "        df = df.withColumn('balance', F.regexp_replace(df['balance'], r\"\\$\", \" \"))\n",
    "\n",
    "        # Then, cast balance column to integer\n",
    "        df = df.withColumn('balance', df['balance'].cast('int'))\n",
    "\n",
    "        # Create new column duration_in_year\n",
    "        # Divide value in duration column with 365, then round down and convert to integer\n",
    "        df = df.withColumn('duration_in_year', F.floor(df['duration'] / 365).cast('int'))\n",
    "\n",
    "        # Arrange columns\n",
    "        df = df.select(\n",
    "            'loan_data_id', \n",
    "            'age', \n",
    "            'job', \n",
    "            'marital_id', \n",
    "            'education_id', \n",
    "            'default', \n",
    "            'balance', \n",
    "            'housing',\n",
    "            'loan',\n",
    "            'contact',\n",
    "            'day',\n",
    "            'month',\n",
    "            'duration',\n",
    "            'duration_in_year',\n",
    "            'campaign',\n",
    "            'days_since_last_campaign',\n",
    "            'previous_campaign_contacts',\n",
    "            'previous_campaign_outcome',\n",
    "            'subscribed_deposit',\n",
    "            'created_at',\n",
    "            'updated_at'\n",
    "        )\n",
    "\n",
    "        print(\"Transformation process successful for table: marketing_campaign_deposit\")\n",
    "\n",
    "        # Set success log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"transformation\", \"success\", \"source_db\", \"marketing_campaign_deposit\", current_timestamp)]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\"])\n",
    "    \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Transformation process failed: {e}\")\n",
    "\n",
    "        # Set failed log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"transformation\", \"failed\", \"source_db\", \"marketing_campaign_deposit\", current_timestamp)]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "\n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a9d2dab-f948-4d0d-be77-62be5c32ee45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation process successful for table: marketing_campaign_deposit\n"
     ]
    }
   ],
   "source": [
    "transformed_marketing_df = transform_marketing_campaign(spark=spark, df=marketing_campaign_deposit_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40916d0-6184-4f76-953b-e82dd2fc6650",
   "metadata": {},
   "source": [
    "### Transform *customers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30c2345e-fae8-4484-99e1-9e4b418842af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "|TransactionID|CustomerID|CustomerDOB|CustGender|CustLocation|CustAccountBalance|TransactionDate|TransactionTime|TransactionAmount (INR)|\n",
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "|      T642232|  C1010028|    25/8/88|         F|       DELHI|         296828.37|        29/8/16|          95212|                    557|\n",
      "|       T87414|  C1010035|     2/3/92|         M|      MUMBAI|           7284.42|         1/8/16|         111917|                     50|\n",
      "|      T560676|C1010035_2|     9/6/80|         M| NAVI MUMBAI|         378013.09|        27/8/16|         185011|                    700|\n",
      "|      T610204|  C1010036|    26/2/96|         M|     GURGAON|         355430.17|        26/8/16|          95203|                    208|\n",
      "|      T957663|  C1010041|     6/9/93|         F|       DELHI|          34119.48|        10/9/16|         162533|                  14500|\n",
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7f4b89b-2b94-4a9a-91d5-61789c440eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, to_date, date_format, year, round\n",
    "\n",
    "def transform_customers(spark, df):\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Rename selected columns\n",
    "        rename_columns = {\n",
    "            \"CustomerID\" : \"customer_id\",\n",
    "            \"CustomerDOB\" : \"birth_date\",\n",
    "            \"CustGender\" : \"gender\",\n",
    "            \"CustLocation\" : \"location\",\n",
    "            \"CustAccountBalance\" : \"account_balance\"\n",
    "        }\n",
    "        \n",
    "        df = df.withColumnsRenamed(rename_columns)\n",
    "\n",
    "        # Mapping gender column value\n",
    "        df = df.withColumn(\"gender\", \n",
    "                           when(col(\"gender\") == \"M\", \"Male\")\n",
    "                           .when(col(\"gender\") == \"F\", \"Female\")\n",
    "                           .when(col(\"gender\") == \"T\", \"Other\")\n",
    "                           .otherwise(col(\"gender\"))\n",
    "                          )\n",
    "\n",
    "        # Convert to DATE format\n",
    "        df = df.withColumn(\"birth_date\", \n",
    "                           to_date(col(\"birth_date\"), \"d/M/yy\")\n",
    "                          )\n",
    "\n",
    "        # # Give flag if there's a year that > 2025\n",
    "        df = df.withColumn(\"birth_date\", \n",
    "                           when(year(col(\"birth_date\")) > 2025, lit(None))\n",
    "                           .otherwise(col(\"birth_date\"))\n",
    "                          )\n",
    "\n",
    "        # Cast account_balance column to float\n",
    "        df = df.withColumn(\"account_balance\", round(df[\"account_balance\"].cast(\"float\")))\n",
    "\n",
    "        # Arrange columns\n",
    "        df = df.select(\n",
    "            'customer_id', \n",
    "            'birth_date', \n",
    "            'gender', \n",
    "            'location', \n",
    "            'account_balance' \n",
    "        )\n",
    "        \n",
    "        print(\"Transformation process successful for table: customers\")\n",
    "\n",
    "        # Set success log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"transformation\", \"success\", \"csv\", \"customers\", current_timestamp)]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\"])\n",
    "    \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Transformation process failed: {e}\")\n",
    "\n",
    "        # Set failed log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"transformation\", \"failed\", \"csv\", \"customers\", current_timestamp)]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "\n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b033d9e3-4623-49b2-b78e-4e4508db2a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation process successful for table: customers\n"
     ]
    }
   ],
   "source": [
    "transformed_customers_df = transform_customers(spark=spark, df=bank_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93206007-080e-4b63-b750-debaef659f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+-----------+---------------+\n",
      "|customer_id|birth_date|gender|   location|account_balance|\n",
      "+-----------+----------+------+-----------+---------------+\n",
      "|   C1010028|1988-08-25|Female|      DELHI|       296828.0|\n",
      "|   C1010035|1992-03-02|  Male|     MUMBAI|         7284.0|\n",
      "| C1010035_2|1980-06-09|  Male|NAVI MUMBAI|       378013.0|\n",
      "|   C1010036|1996-02-26|  Male|    GURGAON|       355430.0|\n",
      "|   C1010041|1993-09-06|Female|      DELHI|        34119.0|\n",
      "| C1010041_2|1975-09-14|Female|      NOIDA|       746732.0|\n",
      "| C1010041_3|1992-07-13|Female|      LOHIT|         1291.0|\n",
      "|   C1010064|1988-09-19|  Male|      DELHI|            2.0|\n",
      "|   C1010065|1985-02-16|  Male|  BARABANKI|        18819.0|\n",
      "|   C1010071|1985-09-14|Female|  NEW DELHI|       329868.0|\n",
      "|   C1010074|1985-09-14|Female|  NEW DELHI|       329868.0|\n",
      "|   C1010078|1983-09-16|Female|     MUMBAI|      1421750.0|\n",
      "|   C1010129|1992-09-15|  Male|    KOLKATA|        27340.0|\n",
      "| C1010129_2|1997-12-22|  Male|     NAGPUR|        16202.0|\n",
      "|   C1010145|1992-09-15|  Male|    KOLKATA|        27340.0|\n",
      "|   C1010211|1990-06-25|  Male|    KOLKATA|        64539.0|\n",
      "|   C1010212|1955-04-29|Female|   DURGAPUR|           41.0|\n",
      "|   C1010220|1984-05-24|  Male|    GURGAON|        47601.0|\n",
      "|   C1010230|1984-12-12|Female|    GURGAON|        83698.0|\n",
      "|   C1010334|1979-07-23|  Male|     MUMBAI|        27240.0|\n",
      "+-----------+----------+------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_customers_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e940fd-31d9-4cd5-9ff2-e816068cdbde",
   "metadata": {},
   "source": [
    "### Transform *transaction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22997495-89d0-49c4-98d4-cdcbed6c207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, to_date, date_format, from_unixtime\n",
    "\n",
    "def transform_transactions(spark, df):\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Rename selected columns\n",
    "        rename_columns = {\n",
    "            \"TransactionID\" : \"transaction_id\",\n",
    "            \"CustomerID\" : \"customer_id\",\n",
    "            \"TransactionDate\" : \"transaction_date\",\n",
    "            \"TransactionTime\" : \"transaction_time\",\n",
    "            \"TransactionAmount (INR)\" : \"transaction_amount\"\n",
    "        }\n",
    "        \n",
    "        df = df.withColumnsRenamed(rename_columns)\n",
    "\n",
    "        # Convert transaction_date column value to DATE format\n",
    "        df = df.withColumn(\"transaction_date\", \n",
    "                           to_date(col(\"transaction_date\"), \"d/M/yy\")\n",
    "                          )\n",
    "\n",
    "        # Give flag if there's a year that > 2025\n",
    "        df = df.withColumn(\"transaction_date\", \n",
    "                           when(year(col(\"transaction_date\")) > 2025, lit(None))\n",
    "                           .otherwise(col(\"transaction_date\"))\n",
    "                          )\n",
    "\n",
    "        # Convert transaction_time column value to HH:MM:SS format\n",
    "        df = df.withColumn(\"transaction_time\",\n",
    "                           from_unixtime(col(\"transaction_time\").cast(\"int\"), \"HH:mm:ss\")\n",
    "                          )\n",
    "\n",
    "        # Cast transaction_amount column to float\n",
    "        df = df.withColumn(\"transaction_amount\", round(df[\"transaction_amount\"].cast(\"float\")))\n",
    "                          \n",
    "        # Arrange columns\n",
    "        df = df.select(\n",
    "            'transaction_id', \n",
    "            'customer_id', \n",
    "            'transaction_date', \n",
    "            'transaction_time', \n",
    "            'transaction_amount' \n",
    "        )\n",
    "        \n",
    "        print(\"Transformation process successful for table: transactions\")\n",
    "\n",
    "        # Set success log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"transformation\", \"success\", \"csv\", \"transactions\", current_timestamp)]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\"])\n",
    "    \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Transformation process failed: {e}\")\n",
    "\n",
    "        # Set failed log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"transformation\", \"failed\", \"csv\", \"transactions\", current_timestamp)]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "\n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65ad6514-785f-40ce-8f96-d97c9c1ea238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation process successful for table: transactions\n"
     ]
    }
   ],
   "source": [
    "transformed_transactions_df = transform_transactions(spark=spark, df=bank_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c9d45c8-700b-4502-856a-03082e434432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------------+----------------+------------------+\n",
      "|transaction_id|customer_id|transaction_date|transaction_time|transaction_amount|\n",
      "+--------------+-----------+----------------+----------------+------------------+\n",
      "|       T642232|   C1010028|      2016-08-29|        02:26:52|             557.0|\n",
      "|        T87414|   C1010035|      2016-08-01|        07:05:17|              50.0|\n",
      "|       T560676| C1010035_2|      2016-08-27|        03:23:31|             700.0|\n",
      "|       T610204|   C1010036|      2016-08-26|        02:26:43|             208.0|\n",
      "|       T957663|   C1010041|      2016-09-10|        21:08:53|           14500.0|\n",
      "|       T113533| C1010041_2|      2016-08-06|        23:38:57|            2397.0|\n",
      "|       T888200| C1010041_3|      2016-09-07|        00:24:19|              20.0|\n",
      "|       T152889|   C1010064|      2016-08-05|        07:45:05|            3000.0|\n",
      "|       T670148|   C1010065|      2016-08-28|        21:03:51|             500.0|\n",
      "|       T601310|   C1010071|      2016-08-26|        05:44:03|            1121.0|\n",
      "|       T870991|   C1010074|      2016-09-08|        02:10:28|            6800.0|\n",
      "|        T83940|   C1010078|      2016-08-01|        19:50:57|             124.0|\n",
      "|       T355205|   C1010129|      2016-08-14|        04:35:38|            1000.0|\n",
      "|       T982281| C1010129_2|      2016-09-15|        04:55:42|              26.0|\n",
      "|       T714372|   C1010145|      2016-09-03|        23:51:42|             400.0|\n",
      "|        T13481|   C1010211|      2016-09-25|        14:05:54|            2209.0|\n",
      "|       T209811|   C1010212|      2016-09-03|        12:49:02|             431.0|\n",
      "|       T269233|   C1010220|      2016-08-12|        23:15:03|             550.0|\n",
      "|       T940787|   C1010230|      2016-09-11|        06:23:33|            1823.0|\n",
      "|       T591335|   C1010334|      2016-08-26|        03:20:58|            1165.0|\n",
      "+--------------+-----------+----------------+----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_transactions_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3a2c2-7b61-4ed6-8cba-68271a064913",
   "metadata": {},
   "source": [
    "## **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd196f16-8462-49da-bb12-7e86fb1a6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "def dwh_engine_sqlalchemy():\n",
    "    return create_engine(f\"postgresql://{DWH_DB_USER}:{DWH_DB_PASS}@{DWH_DB_HOST}:{DWH_DB_PORT}/{DWH_DB_NAME}\")\n",
    "\n",
    "def load_to_dwh(spark, df, table_name, source_name):\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Establish connection to warehouse db\n",
    "        conn = dwh_engine_sqlalchemy()\n",
    "\n",
    "        with conn.begin() as connection:\n",
    "\n",
    "            # Truncate all tables in data warehouse\n",
    "            connection.execute(text(f\"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE\"))\n",
    "\n",
    "        print(f\"Success truncating table: {table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error when truncating table: {e}\")\n",
    "\n",
    "        log_message = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"load\", \"failed\", source_name, table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        \n",
    "        load_log_msg(spark=spark, log_msg=log_message) \n",
    "\n",
    "    finally:\n",
    "        conn.dispose()\n",
    "\n",
    "    # Load transformed DataFrame to warehouse db\n",
    "    try:\n",
    "        \n",
    "        # Getdwh db config\n",
    "        DWH_DB_URL, DWH_DB_USER, DWH_DB_PASS = dwh_engine()\n",
    "    \n",
    "        # Set config\n",
    "        properties = {\n",
    "            \"user\" : DWH_DB_USER,\n",
    "            \"password\" : DWH_DB_PASS,\n",
    "        }\n",
    "\n",
    "        df.write.jdbc(url=DWH_DB_URL,\n",
    "                      table=table_name,\n",
    "                      mode=\"append\",\n",
    "                      properties=properties)\n",
    "\n",
    "        print(f\"Load process successful for table: {table_name}\")\n",
    "\n",
    "        log_message = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"load\", \"success\", source_name, table_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        load_log_msg(spark=spark, log_msg=log_message) \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Load process failed: {e}\")\n",
    "\n",
    "        log_message = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"load\", \"failed\", source_name, table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        \n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f26df60-41ce-47f1-bede-b0d81db607ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: customers\n",
      "Load process successful for table: customers\n"
     ]
    }
   ],
   "source": [
    "load_to_dwh(spark=spark, df=transformed_customers_df, table_name=\"customers\", source_name=\"source_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "828a2731-6fc5-4cd0-b2d3-ced78973c088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: transactions\n",
      "Load process successful for table: transactions\n"
     ]
    }
   ],
   "source": [
    "load_to_dwh(spark=spark, df=transformed_transactions_df, table_name=\"transactions\", source_name=\"source_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "945ee479-584f-4dc3-9234-0dd2538332c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: education_status\n",
      "Load process successful for table: education_status\n"
     ]
    }
   ],
   "source": [
    "load_to_dwh(spark=spark, df=education_status_df, table_name=\"education_status\", source_name=\"source_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0de0f336-57c2-4135-ae70-e9573cb4fe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: marital_status\n",
      "Load process successful for table: marital_status\n"
     ]
    }
   ],
   "source": [
    "load_to_dwh(spark=spark, df=marital_status_df, table_name=\"marital_status\", source_name=\"source_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "250f1a73-a641-4d0e-8b12-4430d46a3f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: marketing_campaign_deposit\n",
      "Load process successful for table: marketing_campaign_deposit\n"
     ]
    }
   ],
   "source": [
    "load_to_dwh(spark=spark, df=transformed_marketing_df, table_name=\"marketing_campaign_deposit\", source_name=\"source_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22282f5b-f4d6-40d1-9a7d-1dccaaf84c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
